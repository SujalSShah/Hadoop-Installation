1. First you need to perform some prerequisites
sudo apt-get install openjdk-8-jdk-headless
sudo apt install openjdk-8-jdk-headless
sudo addgroup hadoop
sudo adduser --ingroup hadoop hduser
sudo usermod -aG sudo hduser
su hduser
sudo ssh-keygen -t rsa -P ""
sudo cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

2. Now you need to disable IPv6. Open the /etc/sysctl.conf file and add the following lines to the end of the file and save it. (One way of opening the file is sudo nano /etc/sysctl.conf, after you add the lines you need to press Ctrl+X, Shift Y and Enter)

net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1

We need to restart the system at this point.

3. Now we download hadoop
cd /usr/local
sudo wget https://mirrors.estointernet.in/apache/hadoop/core/hadoop-3.3.0/hadoop-3.3.0.tar.gz
sudo tar xzf hadoop-3.3.0.tar.gz
sudo mv hadoop-3.3.0 hadoop
sudo chown -R hduser:hadoop hadoop

4. Now open sudo nano $HOME/.bashrc and add the following lines:
export HADOOP_HOME=/usr/local/hadoop
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64
unalias fs &> /dev/null
alias fs="hadoop fs"
unalias hls &> /dev/null
alias hls="fs -ls"
lzohead () {
    hadoop fs -cat $1 | lzop -dc | head -1000 | less
}
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:/usr/local/hadoop/sbin/

5. Enter following commands:
source ~/.bashrc
cd /usr/local/hadoop/etc/hadoop

6. Add the following line to hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64

7. Run the following commands:
sudo mkdir -p /app/hadoop/tmp
sudo chown hduser:hadoop /app/hadoop/tmp

8. Make the following changes in core-site.xml (this file is present in /usr/local/hadoop/etc/hadoop)
Add the following between <configuration> and </configuration>
<property>
  <name>hadoop.tmp.dir</name>
  <value>/app/hadoop/tmp</value>
  <description>A base for other temporary directories.</description>
</property>

<property>
  <name>fs.default.name</name>
  <value>hdfs://localhost:54310</value>
  <description>The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.</description>
</property>

9. In the file mapred-site.xml Add the following between <configuration> and </configuration>
<property>
  <name>mapred.job.tracker</name>
  <value>localhost:54311</value>
  <description>The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  </description>
</property>

10. In the file hdfs-site.xml Add the following between <configuration> and </configuration>
<property>
  <name>dfs.replication</name>
  <value>1</value>
  <description>Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
</property>

11. Finally we format namenode by the following commands at /usr/local/hadoop/sbin   :
sudo chmod 777 /usr/local/hadoop
hadoop namenode -format




THUS WE SUCCESSFULLY INSTALLED HADOOP.





Now to start hadoop, we need to run command:
ssh localhost

(If ssh shows connection refused error, then follow:
sudo apt-get remove openssh-client openssh-server
sudo apt-get install openssh-client openssh-server
sudo apt-get install ssh
)

This command is to start hadoop services:
/usr/local/hadoop/sbin/start-all.sh

This command is to check that all hadoop services are running (6 services should appear):
jps

This command is to stop hadoop services:
/usr/local/hadoop/sbin/stop-all.sh